I will implement a specialized trajectory tracking system for Sugarscape simulations, tailored for RL post-training.

### 1. Design: `SugarTrajectory`
I will create `redblackbench/sugarscape/trajectory.py` with a structure optimized for the Sugarscape environment.
*   **Structure**: Similar to `GameTrajectory` but adapted for a spatial grid world.
*   **State Representation**:
    *   **Grid**: Sparse or full representation of Sugar/Spice/Agent positions.
    *   **Agent State**: Position, Wealth, Vision, Metabolism, History.
*   **Action Space**:
    *   **Move**: Target coordinate `(x, y)`.
    *   **Reasoning**: The thought trace generated by the LLM.
*   **Reward**:
    *   **Immediate**: Resource gain (Sugar/Spice harvested).
    *   **Metabolic Cost**: Energy consumed.
    *   **Net**: Gain - Cost.
    *   **Survival**: +1 per tick alive (dense reward).

### 2. Implementation Steps
1.  **Define Data Classes**:
    *   `SugarTimestep`: Captures the grid state and agent states at tick `t`.
    *   `AgentAction`: Records `agent_id`, `prompt` (context), `reasoning` (thought), `move` (action), and `reward`.
    *   `SugarTrajectory`: Aggregates timesteps and actions.
2.  **Collector Integration**:
    *   Update `SugarSimulation` to instantiate a `SugarTrajectory`.
    *   Record state at the start of each `step()`.
    *   Record agent actions inside `LLMSugarAgent` (or via a callback in simulation).
    *   Save to JSON at the end of `run()`.

### 3. RL Data Format
The JSON output will be structured to easily convert into `(S, A, R, S')` tuples:
```json
{
  "timesteps": [
    {
      "tick": 1,
      "agents": { "1": {"pos": [2,3], "wealth": 10, ...} },
      "grid": { ... },
      "actions": [
        {
          "agent_id": 1,
          "context_prompt": "...",  # The exact input to LLM
          "generated_text": "...",  # The exact output (reasoning + move)
          "parsed_move": [3,3],
          "reward": 3
        }
      ]
    }
  ]
}
```

### 4. Integration
*   Modify `SugarSimulation` to initialize and update the trajectory.
*   Modify `LLMSugarAgent` to return the full prompt and response so it can be logged (currently it just returns the move or modifies state).

This design ensures we capture the exact inputs and outputs of the model, which is critical for fine-tuning (SFT) or RL (PPO/DPO).
